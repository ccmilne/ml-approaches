{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWTe9VS3_b11"
   },
   "source": [
    "## SI 670 Applied Machine Learning, Week 5:  One-hot encoding, calibration, decision trees, random forests. (Due Thursday October 14 2021, 12:59pm.)\n",
    "\n",
    "Total: 80 points\n",
    "Question 1: 30 points\n",
    "Question 2: 20 points\n",
    "Question 3: 30 points \n",
    "\n",
    "Correct answers and code receive full credit, but partial credit will be awarded if you have the right idea even if your final answers aren't quite right.\n",
    "\n",
    "Submit your completed notebook file to the Canvas site. IMPORTANT: please name your submitted file si670-hw5-youruniqname.ipynb.\n",
    "\n",
    "As a reminder, the notebook code you submit must be your own work. Feel free to discuss general approaches to the homework with classmates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vIZegu_3GU0"
   },
   "source": [
    "### Put your name here: Cameron Milne\n",
    "\n",
    "### Put your uniquename here: ccmilne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (30 points)\n",
    "\n",
    "This question doesn't require coding, in the sense of writing programs that do all the computing of the answer for you: you can figure out the answer by hand and then put your answers into the notebook by defining the correct numpy array in python. Alternatively you could write out your answers in markdown using simple LaTeX tags. (See [here](https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/) for how to write things with math operators in LaTeX format.)\n",
    "\n",
    "\n",
    "#### (a) (10 points) One-hot encoding\n",
    "\n",
    "If you have a dataset with three data points, and each data point has three features. Among them, X2 and X3 are categorical variables:\n",
    "\n",
    "|    X1\t|  X2 \t|  X3\t|\n",
    "|----\t|----\t|----\t|\n",
    "|   1.3\t|  a \t| a \t|\n",
    "|   0.7 |  b \t| c \t|\n",
    "|   2.1 |  a \t| b     |\n",
    "\n",
    "Please manually convert this dataset into numerical format with the categorical variables transformed into one-hot encoding. Please keep the order of X1, X2, and X3, and use alphabetical order for the one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "|  X1\t|  X2_a |  X2_b\t| X3_a | X3_b | X3_c | \n",
    "|----\t| ----\t|----\t|---   |----  | ---- |\n",
    "|   1.3\t|   1 \t|  0 \t|  1   |  0   | 0\n",
    "|   0.7 |   0 \t|  1 \t|  0   |  0   | 1\n",
    "|   2.1 |   1 \t|  0    |  0   |  1   | 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (b) (10 points) Calibration\n",
    "Recall the calibration curve has the predicted probability as its x-axis and the true probability (also known as the \"empirical probability\" as its y-axis. Suppose you are given a binary classifier and its predicted probabilities on a test set with 15 data points. The labels of these data points are also given. Please calculate the true probabilities in three bins: \\[0, 0.3), \\[0.3, 0.7), \\[0.7, 1\\]. You could further use these probabilities to draw a calibration curve but it's not required for this question. You only need to give the 3 numbers indicating the true probabilities for each bin.  It might help to recall the weather example from class: the \"true\" empirical probability of rain is just the fraction of times it *actually* rained according to the data, for a given predicted probability (or range of probabilities) from the weatherperson.\n",
    "\n",
    "|Predicted probability | Label |\n",
    "|----\t               |----   |\n",
    "|   0.40               |   0   |\n",
    "|   0.77               |   1   |\n",
    "|   0.84               |   0   |\n",
    "|   0.68               |   0   |\n",
    "|   0.73               |   1   |\n",
    "|   0.88               |   1   |\n",
    "|   0.69               |   0   |\n",
    "|   0.24               |   0   |\n",
    "|   0.70               |   1   |\n",
    "|   0.41               |   1   |\n",
    "|   0.34               |   1   |\n",
    "|   0.18               |   1   |\n",
    "|   0.31               |   1   |\n",
    "|   0.58               |   1   |\n",
    "|   0.00               |   0   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer:\n",
    "\n",
    "- Bin [0, 0.3): 3 instances, only one rain --> 33%\n",
    "- Bin [0.3, 0.7): 7 instances, only one rain --> 57%\n",
    "- Bin [0.7, 1]: 5 instances, only one rain --> 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E6z1mGZ3GU1",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### (c) (10 points) Random forest parameters\n",
    "\n",
    "Suppose your current random forest classifier is facing an overfitting situation. Please state whether increasing or decreasing the following parameters can potentially help you reduce overfitting or not, and why?\n",
    "\n",
    "(i) `n_estimators`\n",
    "\n",
    "answer: If there are more estimators, a model is less likely to overfit. Increasing the number of trees that are to be built before assessing (taking the maximum voting or averages of predictions) will help to simplify the model and reduce overfitting.\n",
    "\n",
    "(ii) `max_features`\n",
    "\n",
    "answer: Max features represents the number of features to consider in looking for the best split. Reducing max features will decrease overfitting.\n",
    "\n",
    "(iii) `max_depth`\n",
    "\n",
    "answer: Max depth represents the depth of each tree; the deeper the tree, the more splits and information is captured. Therefore, tree depth is useful to a point, but can hurt testing performance and in this case, reducing max_depth can reduce overfitting. \n",
    "\n",
    "(iv) `n_jobs`\n",
    "\n",
    "answer: The number of jobs (how many processors are to be used in the engine of the model), will have no effect on overfitting. This parameter affects model training speed \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Qlj8Dm3GU5"
   },
   "source": [
    "### Question 2 (20 points) Cross-validation for very small datasets.\n",
    "\n",
    "In our lecture about data leakage, we talked about one simple strategy to help you avoid data leakage: before you do any work with a new dataset, split off a final test dataset, and use this final test dataset as the very last step in your validation. However, when you have a very small dataset, one issue is that it leads to really tiny test sets, which leads to unreliable test evaluation scores. For example, if you have a dataset with a total of 60 samples, and hold back 25% as a final test set, you'll get a final test set with 15 samples. In this case, a single evaluation score based on merely 15 samples could be very unreliable and probably not something to be relied on heavily.\n",
    "\n",
    "To make evaluation more reliable, we discussed how people usually use *cross-validation* to generate *multiple* evaluation scores, each on a different train/test split of the data. That is, you split the train and test set multiple times and then calculate the average of the resulting test scores. This is the approach we'll use to estimate a more reliable final test set score.  We don't want to use these final test sets to also tune our hyperparameters (to avoid data leakage), so we make sure to learn the model and tune the hyperparameters using only the data in the training split.  To do that, we do a second split *within the training data split* so that we have (i) a training set for the model and (ii) a separate *validation split* that's used to evaluate the model and pick the best setting for the hyperparameters.\n",
    "\n",
    "Here's the general recipe:\n",
    "\n",
    "1. Split the whole dataset into $k$ equal folds\n",
    "2. For $i$ from 1 to $k$   (for each of the $k$ folds)\n",
    "\n",
    "    a. Take the $i$-th fold as a final test set. \n",
    "    \n",
    "    b. With the remaining data (i.e. combining the other folds), apply a standard train/test split (75%/25%).\n",
    "    \n",
    "    c. For each possible tuning value of hyperparameter (in our case, alpha):\n",
    "    \n",
    "       - Train the model with the 75% part using a specific hyperparameter choice (for alpha)\n",
    "             \n",
    "       - Evaluate the model with the 25% part. This 25% is our \"validation set\".\n",
    "             \n",
    "       Pick the hyperparameter value that gave the best score on the validation set.\n",
    "            \n",
    "    d. Once you find the optimal hyperparameter, do the test set predictions on this $i$-th fold and calculate the test score for this $i$-th fold.\n",
    "  \n",
    "3. Report the average of the final test set scores you got across all $k$ folds.\n",
    "\n",
    "To simulate a small dataset scenario, we have provided the code that selects the first 60 samples from the built-in boston dataset.. We've also given you the variable 'alpha_list', which has the range of ridge regression hyperparameter alphe you should use for tuning. \n",
    "\n",
    "Write the code that implements the above scheme on this subset of the boston dataset. You can split the whole dataset into k folds (step 1) by using the handy `KFold` function (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html). For each fold, you can then use the default `train_test_split` to get that's fold's training and validation data (step 2b). With this training data, train a Ridge regression model, and use the validation set to evaluate, tuning to find the optimal hyper-parameter alpha value within each fold (step 2c). Get the final test set score using this optimal model (step 2d). Repeat for all $k$ folds to obtain a set of final test scores.\n",
    "\n",
    "Finally, you need to return the mean value of the $k$ final test scores. This is your final (more reliable) test set prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AC8gYVpi3GU6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.009767010370020058"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    from sklearn.datasets import load_boston\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import KFold\n",
    "    from matplotlib import pyplot as plt\n",
    "    import operator\n",
    "    import numpy as np\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    X,y = load_boston(return_X_y=True)\n",
    "    print(X)\n",
    "    \n",
    "    X=X[:60,:]\n",
    "    y=y[:60]\n",
    "    alpha_list = [0.001,0.01,0.1,1,10]\n",
    "    \n",
    "    print(type(X))\n",
    "    \n",
    "    # Your code here\n",
    "    final_test_scores = []\n",
    "\n",
    "    K = 5\n",
    "    kf = KFold(n_splits=K)     \n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train , X_test = X[train_index], X[test_index]\n",
    "        y_train , y_test = y[train_index], y[test_index]\n",
    "        X_train_validate, X_test_validate, y_train_validate, y_test_validate = train_test_split(X_train, y_train, train_size=0.75, random_state=7)\n",
    "\n",
    "        alphas = {}\n",
    "        for a in alpha_list:\n",
    "            model = Ridge(alpha=a).fit(X_train_validate, y_train_validate)\n",
    "            alphas[str(a)] = model.score(X_test_validate, y_test_validate)\n",
    "            # print(model.score(X_test_validate, y_test_validate))\n",
    "            \n",
    "        top = sorted(alphas.items(), key=operator.itemgetter(1),reverse=True)[:1][0]\n",
    "        \n",
    "        new_ridge = Ridge(alpha=top[0]).fit(X_train, y_train)\n",
    "        score = new_ridge.score(X_test, y_test)\n",
    "\n",
    "        final_test_scores.append(score)\n",
    "    \n",
    "    mean_test_score = sum(final_test_scores) / len(final_test_scores)\n",
    "    return mean_test_score\n",
    "    \n",
    "answer_two()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMn9p67w3GU8"
   },
   "source": [
    "### Question 3 (30 points)  Decision trees.\n",
    "\n",
    "For this question, we'll work with the Statlog (German Credit Data) dataset that classifies people described by a set of attributes as good or bad credit risks. Download the dataset from [here](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) (Use the german.data). The last column is the prediction target and the remaining columns are features. \n",
    "\n",
    "\n",
    "\n",
    "(a) (20 points) First transform the categorical features into one-hot encodings. Then train a decision tree classifier and a random forest classifier. You should return 6 items as follows: the trained decision tree classifier, the trained random forest classifier, decision tree training accuracy, decision tree test accuracy,  random forest training accuracy, random forest test accuracy. **Please use random_state = 0 for train_test_split, decision tree classifier, and random forest classifier.**\n",
    "\n",
    "*Hint 1: The columns of categorical features are 0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19.*\n",
    "\n",
    "*Hint 2: You may have a problem using `OneHotEncoder` to handle string values and numerical values at the same time. You can transform the string columns first, and then concatenate with the numerical features.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DecisionTreeClassifier(random_state=0),\n",
       " RandomForestClassifier(random_state=0),\n",
       " 1.0,\n",
       " 0.676,\n",
       " 1.0,\n",
       " 0.78)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three_a():\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pandas as pd\n",
    "    \n",
    "    german = pd.read_csv('german.data', header=None, sep=' ')\n",
    "    X = german.iloc[:,:-1]\n",
    "    y = german.iloc[:,-1]\n",
    "    \n",
    "    ct = ColumnTransformer([(\"OneHote\", OneHotEncoder(), [0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19])], remainder='passthrough')\n",
    "    transformed = ct.fit_transform(X)\n",
    "    # print(ct.transformers_) #proof that it's keeping those columns\n",
    "\n",
    "    ### Decision Tree\n",
    "    X_train, X_test, y_train, y_test = train_test_split(transformed, y, random_state=0)\n",
    "    DT_classifier = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n",
    "    train_score_DT = DT_classifier.score(X_train, y_train)\n",
    "    test_score_DT = DT_classifier.score(X_test, y_test)\n",
    "    \n",
    "    ### Random Forest\n",
    "    X_train, X_test, y_train, y_test = train_test_split(transformed, y, random_state=0)\n",
    "    RF_classifier = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
    "    train_score_RF = RF_classifier.score(X_train, y_train)\n",
    "    test_score_RF = RF_classifier.score(X_test, y_test)\n",
    "    \n",
    "    return DT_classifier, RF_classifier, train_score_DT, test_score_DT, train_score_RF, test_score_RF\n",
    "answer_three_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGHnU1TVHUiX"
   },
   "source": [
    "(b) (10 points) For the Decision Tree Classifier, compute **feature importance** that comes with decision tree classifier in scikit-learn to get the top three most important features. Also do it for the random forest. Are they the same sets of features? Or do you have any interesting findings and comments?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4q1z4UatHT6x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(55, 0.13576646088784447),\n",
       "  (58, 0.1016517719679399),\n",
       "  (3, 0.09234069495653098)],\n",
       " [(55, 0.09442579022165769),\n",
       "  (54, 0.07906771820549399),\n",
       "  (58, 0.07836738053320748)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three_b():\n",
    "    import numpy as np\n",
    "    import operator\n",
    "    DT_classifier, RF_classifier, _,_,_,_ = answer_three_a()\n",
    "    \n",
    "    # your code here\n",
    "    DT = {}\n",
    "    for i, feat in enumerate(DT_classifier.feature_importances_):\n",
    "        DT[i] = feat\n",
    "    DT_important_features = sorted(DT.items(), key=operator.itemgetter(1), reverse=True)[:3]\n",
    "\n",
    "    RF = {}\n",
    "    for i, feat in enumerate(RF_classifier.feature_importances_):\n",
    "        RF[i] = feat    \n",
    "    RF_important_features = sorted(RF.items(), key=operator.itemgetter(1), reverse=True)[:3]\n",
    "\n",
    "    return DT_important_features, RF_important_features \n",
    "    # DT_important_features is np.array of 3 numbers indicates the indices of top 3 important features in the training features after onehot encoding.\n",
    "    # DT_important_features is np.array of 3 numbers indicates the indices of top 3 important features in the training features after onehot encoding.\n",
    "\n",
    "answer_three_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the Decision Tree Classifier and the Random Forest, two of the same feature indices are within the top three results (55 and 58). Across models, these features are suggestedly important in shaping the outcome."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "si670f21_hw_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
