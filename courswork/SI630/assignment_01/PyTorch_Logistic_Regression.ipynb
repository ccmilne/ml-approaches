{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### term frequency\n",
    "\n",
    "emails = list(train['email_text'])\n",
    "email_count = len(emails)\n",
    "\n",
    "#Build Term Frequency Dictionary\n",
    "cnt = Counter()\n",
    "for email in tqdm(emails):\n",
    "    tokens = better_tokenize(email)\n",
    "    for t in tokens:\n",
    "        cnt[t] += 1\n",
    "        \n",
    "#Remove rare words\n",
    "minimum_word_frequency = 10\n",
    "only_common = Counter({k: v for k, v in cnt.items() if v >= minimum_word_frequency})\n",
    "\n",
    "\n",
    "\n",
    "### \n",
    "\n",
    "docs = {}\n",
    "\n",
    "counter = 1\n",
    "for email in tqdm(emails):\n",
    "    tokens = better_tokenize(email)    \n",
    "    for t in tokens:\n",
    "        if t not in only_common.keys(): tokens.remove(t)\n",
    "    docs[counter] = tokens\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "### Create Vocab \n",
    "\n",
    "# n_nonzero = 0\n",
    "# vocab = set()\n",
    "\n",
    "# for docterms in docs.values():\n",
    "#     unique_terms = set(docterms)    # all unique terms of this doc\n",
    "#     vocab |= unique_terms           # set union: add unique terms of this doc\n",
    "#     n_nonzero += len(unique_terms)  # add count of unique terms in this doc\n",
    "\n",
    "# docnames = np.array(list(docs.keys()))\n",
    "# vocab = np.array(list(vocab))\n",
    "# vocab_sorter = np.argsort(vocab)\n",
    "\n",
    "# ndocs = len(docnames) #Matrix rows\n",
    "# nvocab = len(vocab) #Matrix columns\n",
    "\n",
    "# data = np.empty(n_nonzero, dtype=np.intc)\n",
    "# rows = np.empty(n_nonzero, dtype=np.intc)\n",
    "# cols = np.empty(n_nonzero, dtype=np.intc)\n",
    "\n",
    "### Create Matrix\n",
    "\n",
    "# Takes in docs, \n",
    "\n",
    "# current_index = 0\n",
    "\n",
    "# for docname, terms in tqdm(docs.items()):\n",
    "#     term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "#     uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "#     n_vals = len(uniq_indices)\n",
    "#     ind_end = current_index + n_vals\n",
    "\n",
    "#     data[current_index:ind_end] = counts\n",
    "#     cols[current_index:ind_end] = uniq_indices\n",
    "#     doc_idx = np.where(docnames == docname)\n",
    "#     rows[current_index:ind_end] = np.repeat(doc_idx, n_vals)\n",
    "\n",
    "#     current_index = ind_end\n",
    "\n",
    "# sparse_matrix = sparse.csr_matrix((data, (rows, cols)), shape=(ndocs, nvocab), dtype=np.intc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def term_frequency(df, minimum_word_frequency=10):\n",
    "#     emails = list(df['email_text'])\n",
    "    \n",
    "#     #Build Term Frequency Dictionary\n",
    "#     cnt = Counter()\n",
    "#     for email in tqdm(emails):\n",
    "#         tokens = better_tokenize(email)\n",
    "#         for t in tokens:\n",
    "#             cnt[t] += 1\n",
    "\n",
    "#     #Remove rare words\n",
    "#     minimum_word_frequency = 10\n",
    "#     only_common = Counter({k: v for k, v in cnt.items() if v >= minimum_word_frequency})\n",
    "    \n",
    "#     return only_common\n",
    "    \n",
    "# term_freq = term_frequency(df=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails = list(train['email_text'])\n",
    "# email_count = len(emails)\n",
    "\n",
    "# #Build Term Frequency Dictionary\n",
    "# cnt = Counter()\n",
    "# for email in tqdm(emails):\n",
    "#     tokens = better_tokenize(email)\n",
    "#     for t in tokens:\n",
    "#         cnt[t] += 1\n",
    "        \n",
    "# #Remove rare words\n",
    "# minimum_word_frequency = 10\n",
    "# only_common = Counter({k: v for k, v in cnt.items() if v >= minimum_word_frequency})\n",
    "\n",
    "# #Vocabulary Differences\n",
    "# print(f\"Total Vocab: {len(cnt)}\")\n",
    "# print(f\"Common Vocab: {len(only_common)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_docs(df, term_freq):\n",
    "#     emails = list(df['email_text'])\n",
    "    \n",
    "#     docs = {}\n",
    "#     counter = 1\n",
    "    \n",
    "#     for email in tqdm(emails):\n",
    "#         tokens = better_tokenize(email)    \n",
    "#         for t in tokens:\n",
    "#             if t not in term_freq.keys(): tokens.remove(t)\n",
    "#         docs[counter] = tokens\n",
    "#         counter += 1\n",
    "\n",
    "#     return docs\n",
    "\n",
    "# docs = build_docs(train, term_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = {}\n",
    "\n",
    "# counter = 1\n",
    "# for email in tqdm(emails):\n",
    "#     tokens = better_tokenize(email)    \n",
    "#     for t in tokens:\n",
    "#         if t not in only_common.keys(): tokens.remove(t)\n",
    "#     docs[counter] = tokens\n",
    "#     counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_vocab(docs):\n",
    "#     n_nonzero = 0\n",
    "#     vocab = set()\n",
    "\n",
    "#     for docterms in docs.values():\n",
    "#         unique_terms = set(docterms)    # all unique terms of this doc\n",
    "#         vocab |= unique_terms           # set union: add unique terms of this doc\n",
    "#         n_nonzero += len(unique_terms)  # add count of unique terms in this doc\n",
    "\n",
    "#     docnames = np.array(list(docs.keys()))\n",
    "#     vocab = np.array(list(vocab))\n",
    "#     vocab_sorter = np.argsort(vocab)\n",
    "    \n",
    "#     ndocs = len(docnames) #Matrix rows\n",
    "#     nvocab = len(vocab) #Matrix columns\n",
    "    \n",
    "#     return (n_nonzero, vocab, vocab_sorter, ndocs, nvocab)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_nonzero = 0\n",
    "# vocab = set()\n",
    "\n",
    "# for docterms in docs.values():\n",
    "#     unique_terms = set(docterms)    # all unique terms of this doc\n",
    "#     vocab |= unique_terms           # set union: add unique terms of this doc\n",
    "#     n_nonzero += len(unique_terms)  # add count of unique terms in this doc\n",
    "    \n",
    "# docnames = np.array(list(docs.keys()))\n",
    "# vocab = np.array(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_sorter = np.argsort(vocab)\n",
    "\n",
    "# ndocs = len(docnames) #Matrix rows\n",
    "# nvocab = len(vocab) #Matrix columns\n",
    "\n",
    "# data = np.empty(n_nonzero, dtype=np.intc)\n",
    "# rows = np.empty(n_nonzero, dtype=np.intc)\n",
    "# cols = np.empty(n_nonzero, dtype=np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_index = 0\n",
    "\n",
    "# for docname, terms in tqdm(docs.items()):\n",
    "#     term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "#     uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "#     n_vals = len(uniq_indices)\n",
    "#     ind_end = current_index + n_vals\n",
    "    \n",
    "#     data[current_index:ind_end] = counts\n",
    "#     cols[current_index:ind_end] = uniq_indices\n",
    "#     doc_idx = np.where(docnames == docname)\n",
    "#     rows[current_index:ind_end] = np.repeat(doc_idx, n_vals)\n",
    "    \n",
    "#     current_index = ind_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_matrix = sparse.csr_matrix((data, (rows, cols)), shape=(ndocs, nvocab), dtype=np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_sparse_tensor(sparse_matrix):\n",
    "#     coo = sparse_matrix.tocoo()\n",
    "#     indices = torch.LongTensor([coo.row.tolist(), coo.col.tolist(),])\n",
    "#     return torch.sparse.LongTensor(indices, torch.LongTensor(coo.data.astype(np.long))) #np.int32\n",
    "\n",
    "# def to_sparse_tensor(sparse_matrix):\n",
    "#     coo = sparse_matrix.tocoo()\n",
    "    \n",
    "#     indices = np.vstack((coo.row, coo.col))\n",
    "#     values = coo.data\n",
    "    \n",
    "#     i = torch.LongTensor(indices)\n",
    "#     v = torch.FloatTensor(values)\n",
    "    \n",
    "#     return torch.sparse.FloatTensor(i, v, torch.Size(coo.shape))\n",
    "\n",
    "# sparse_tensor_train = to_sparse_tensor(sparse_matrix_train)\n",
    "# sparse_tensor_dev = to_sparse_tensor(sparse_matrix_dev)\n",
    "# sparse_tensor_test = to_sparse_tensor(sparse_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_dev[1]\n",
    "\n",
    "for d in docs_dev:\n",
    "    \n",
    "    run a counter on terms\n",
    "    \n",
    "    create row vector\n",
    "    if term in counter,\n",
    "    \n",
    "    vstack\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
